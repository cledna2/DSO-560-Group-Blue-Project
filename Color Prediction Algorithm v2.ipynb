{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import difflib\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import logging\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>brand</th>\n",
       "      <th>mpn</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>brand_category</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>deleted_at</th>\n",
       "      <th>brand_canonical_url</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 246</th>\n",
       "      <th>Unnamed: 247</th>\n",
       "      <th>Unnamed: 248</th>\n",
       "      <th>Unnamed: 249</th>\n",
       "      <th>Unnamed: 250</th>\n",
       "      <th>Unnamed: 251</th>\n",
       "      <th>Unnamed: 252</th>\n",
       "      <th>Unnamed: 253</th>\n",
       "      <th>Unnamed: 254</th>\n",
       "      <th>Unnamed: 255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01DSRPSZTDW2PGK1YWYXJGKZZ0</td>\n",
       "      <td>FILA</td>\n",
       "      <td>400010319073</td>\n",
       "      <td>Original Fitness Sneakers</td>\n",
       "      <td>Vintage Fitness leather sneakers with logo pri...</td>\n",
       "      <td>TheMensStore/Shoes/Sneakers/LowTop</td>\n",
       "      <td>2019-11-15 23:36:38.98161+00</td>\n",
       "      <td>2019-12-19 20:40:30.786144+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.saksfifthavenue.com/fila-original-...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   product_id brand           mpn                       name  \\\n",
       "0  01DSRPSZTDW2PGK1YWYXJGKZZ0  FILA  400010319073  Original Fitness Sneakers   \n",
       "\n",
       "                                         description  \\\n",
       "0  Vintage Fitness leather sneakers with logo pri...   \n",
       "\n",
       "                       brand_category                    created_at  \\\n",
       "0  TheMensStore/Shoes/Sneakers/LowTop  2019-11-15 23:36:38.98161+00   \n",
       "\n",
       "                      updated_at  deleted_at  \\\n",
       "0  2019-12-19 20:40:30.786144+00         NaN   \n",
       "\n",
       "                                 brand_canonical_url  ... Unnamed: 246  \\\n",
       "0  https://www.saksfifthavenue.com/fila-original-...  ...          NaN   \n",
       "\n",
       "  Unnamed: 247 Unnamed: 248  Unnamed: 249  Unnamed: 250  Unnamed: 251  \\\n",
       "0          NaN          NaN           NaN           NaN           NaN   \n",
       "\n",
       "   Unnamed: 252  Unnamed: 253  Unnamed: 254  Unnamed: 255  \n",
       "0           NaN           NaN           NaN           NaN  \n",
       "\n",
       "[1 rows x 256 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Full+data.csv')\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42373, 14)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce to non nan columns\n",
    "cols = data.columns[:14]\n",
    "data = data.loc[:,cols]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>mpn</th>\n",
       "      <th>brand</th>\n",
       "      <th>brand_category</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>bc_product_id</th>\n",
       "      <th>saleable</th>\n",
       "      <th>details</th>\n",
       "      <th>created_at</th>\n",
       "      <th>brand_canonical_url</th>\n",
       "      <th>notes</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01E5ZXP5H0BTEZT9QD2HRZJ47A</td>\n",
       "      <td>5529544</td>\n",
       "      <td>A.L.C.</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Lennox High Waist Cotton &amp; Linen Pants</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-04-17 15:44:57.785000+00:00</td>\n",
       "      <td>5021</td>\n",
       "      <td>2020-04-17 15:44:57.785000+00:00</td>\n",
       "      <td>True to size. High rise.\\n31\" inseam; 14\" leg ...</td>\n",
       "      <td>2020-04-15 21:59:56.695000+00:00</td>\n",
       "      <td>https://shop.nordstrom.com/s/a-l-c-lennox-high...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High-rise trousers tailored from a cool Italia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   product_id      mpn   brand brand_category  \\\n",
       "0  01E5ZXP5H0BTEZT9QD2HRZJ47A  5529544  A.L.C.        Unknown   \n",
       "\n",
       "                                     name labels  \\\n",
       "0  Lennox High Waist Cotton & Linen Pants     []   \n",
       "\n",
       "                         updated_at  bc_product_id  \\\n",
       "0  2020-04-17 15:44:57.785000+00:00           5021   \n",
       "\n",
       "                           saleable  \\\n",
       "0  2020-04-17 15:44:57.785000+00:00   \n",
       "\n",
       "                                             details  \\\n",
       "0  True to size. High rise.\\n31\" inseam; 14\" leg ...   \n",
       "\n",
       "                         created_at  \\\n",
       "0  2020-04-15 21:59:56.695000+00:00   \n",
       "\n",
       "                                 brand_canonical_url notes  \\\n",
       "0  https://shop.nordstrom.com/s/a-l-c-lennox-high...   NaN   \n",
       "\n",
       "                                         description  \n",
       "0  High-rise trousers tailored from a cool Italia...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import extra data\n",
    "extra_data = pd.read_csv('extra_data.csv')\n",
    "extra_data.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to use from extra_data\n",
    "cols = ['product_id', 'brand', 'name', 'description', 'brand_category', 'brand_canonical_url','details']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_data = extra_data.loc[:,cols]\n",
    "data = data.loc[:,cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate data and extra data\n",
    "full_data = pd.concat([data,extra_data])\n",
    "len(full_data) == len(data) + len(extra_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.fillna('UNKNOWNTOKEN',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will join brand and brand_category together to brand_info for use in predictions\n",
    "\n",
    "#full_data['brand_info'] = full_data['brand']+full_data['brand_category']+ \" \" +full_data['brand_canonical_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id             0\n",
       "brand                  0\n",
       "name                   0\n",
       "description            0\n",
       "brand_category         0\n",
       "brand_canonical_url    0\n",
       "details                0\n",
       "brand_info             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = full_data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "stop = set(STOPWORDS)\n",
    "def remove_stopwords(data_col):\n",
    "    new_list = []\n",
    "    a = data_col\n",
    "    for i in range(0,len(a)):\n",
    "        words = word_tokenize(a[i])\n",
    "        res_words = []\n",
    "        for word in words:\n",
    "            if word not in stop:\n",
    "                res_words.append(word)\n",
    "            sentence = \" \".join(res_words)\n",
    "        new_list.append(sentence)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from each of the 4 columns\n",
    "full_data['description'] = remove_stopwords(full_data['description'])\n",
    "full_data['details'] = remove_stopwords(full_data['details'])\n",
    "full_data['name'] = remove_stopwords(full_data['name'])\n",
    "full_data['brand'] = remove_stopwords(full_data['brand'])\n",
    "full_data['brand_category'] = remove_stopwords(full_data['brand_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to do regex cleaning\n",
    "def reg_clean(data,col):\n",
    "    new_list = []\n",
    "    for i in range(0,len(data)):\n",
    "        #special characters \n",
    "        a = re.sub(r'[^ a-zA-Z0-9]',' ',data.loc[i,col])\n",
    "        #new lines\n",
    "        a = re.sub(r'\\n',' ', a)\n",
    "        #remove multiple spaces by a single space\n",
    "        a = re.sub(r'\\s+',' ',a)\n",
    "        #timestamp\n",
    "        a = re.sub(r'\\b[0-9]{1,}am|[0-9]{1,}pm|[0-9]{4,}|[0-9]ish|1st|2nd|3rd|[0-9]{1,2}th|31st|[0-9]{1,}min(?:utes)?s?|[0-9]{1,}h(?:ou)?rs?|[0-9]{3,}\\b','timestamp',a)\n",
    "        a = re.sub(r'\\b[0-9]{1,}timestamp\\b','timestamp',a)\n",
    "        #any numbers as digit\n",
    "        a = re.sub(r'\\b\\d+\\b','digit',a)\n",
    "        #number followed by a variable\n",
    "        a = re.sub(r'\\b\\d{1,}[a-z]{0,}[0-9]{0,}','varchar',a)\n",
    "        #html codes\n",
    "        a = re.sub(r'<.+?>','html',a)\n",
    "        a = re.sub(r'https|www|com','html',a)\n",
    "        # additional stopwords\n",
    "        a = re.sub(r'\\ba\\b', '', a, flags = re.IGNORECASE)\n",
    "        a = re.sub(r'\\bthis|the\\b', '',a, flags = re.IGNORECASE)\n",
    "        new_list.append(a)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['description'] = remove_stopwords(full_data['description'])\n",
    "full_data['details'] = remove_stopwords(full_data['details'])\n",
    "full_data['name'] = remove_stopwords(full_data['name'])\n",
    "full_data['brand'] = remove_stopwords(full_data['brand'])\n",
    "full_data['brand_category'] = remove_stopwords(full_data['brand_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to do regex cleaning\n",
    "def reg_clean(data,col):\n",
    "    new_list = []\n",
    "    for i in range(0,len(data)):\n",
    "        #special characters \n",
    "        a = re.sub(r'[^ a-zA-Z0-9]',' ',data.loc[i,col])\n",
    "        #new lines\n",
    "        a = re.sub(r'\\n',' ', a)\n",
    "        #remove multiple spaces by a single space\n",
    "        a = re.sub(r'\\s+',' ',a)\n",
    "        #timestamp\n",
    "        a = re.sub(r'\\b[0-9]{1,}am|[0-9]{1,}pm|[0-9]{4,}|[0-9]ish|1st|2nd|3rd|[0-9]{1,2}th|31st|[0-9]{1,}min(?:utes)?s?|[0-9]{1,}h(?:ou)?rs?|[0-9]{3,}\\b','timestamp',a)\n",
    "        a = re.sub(r'\\b[0-9]{1,}timestamp\\b','timestamp',a)\n",
    "        #any numbers as digit\n",
    "        a = re.sub(r'\\b\\d+\\b','digit',a)\n",
    "        #number followed by a variable\n",
    "        a = re.sub(r'\\b\\d{1,}[a-z]{0,}[0-9]{0,}','varchar',a)\n",
    "        #html codes\n",
    "        a = re.sub(r'<.+?>','html',a)\n",
    "        a = re.sub(r'https|www|com','html',a)\n",
    "        # additional stopwords\n",
    "        a = re.sub(r'\\ba\\b', '', a, flags = re.IGNORECASE)\n",
    "        a = re.sub(r'\\bthis|the\\b', '',a, flags = re.IGNORECASE)\n",
    "        new_list.append(a)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform regex cleaning on each column\n",
    "full_data['description'] = reg_clean(full_data,'description')\n",
    "full_data['details'] = reg_clean(full_data,'details')\n",
    "full_data['brand'] = reg_clean(full_data,'brand')\n",
    "full_data['name'] = reg_clean(full_data, 'name')\n",
    "full_data['brand_category'] = reg_clean(full_data,'brand_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to lemmatize columns\n",
    "def lemmatize_sentence(data_col):\n",
    "    new_list = []\n",
    "    a = data_col \n",
    "    for i in range(0,len(a)):\n",
    "        words = word_tokenize(a[i])\n",
    "        res_words = []\n",
    "        for word in words:\n",
    "            res_words.append(lemmatizer.lemmatize(word).strip(string.punctuation))\n",
    "        sentence = \" \".join(res_words)\n",
    "        new_list.append(sentence)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize each column\n",
    "full_data['description'] = lemmatize_sentence(full_data['description'])\n",
    "full_data['details'] = lemmatize_sentence(full_data['details'])\n",
    "full_data['brand'] = lemmatize_sentence(full_data['brand'])\n",
    "full_data['brand_category'] = lemmatize_sentence(full_data['brand_category'])\n",
    "full_data['name'] = lemmatize_sentence(full_data['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to start collocation\n",
    "stopwords_coll = set(stopwords.words('english') + [\".\",'.', \",\",\":\", \"''\", \"'s\", \"'\", \"``\", \"(\", \")\", \"-\",\"timestamp\",\"varchar\",\"html\",\"digit\"])\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopwords_coll\n",
    "def collocation_list(data_col):\n",
    "    new_list = []\n",
    "    for i in range(0,len(data_col)):\n",
    "        words = word_tokenize(data_col[i])\n",
    "        res_words = []\n",
    "        for word in words:\n",
    "            if word not in stopwords_coll:\n",
    "                res_words.append(word)\n",
    "        new_list.append(res_words)\n",
    "    return(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['brand', 'brand_category', 'description', 'name','details']\n",
    "n_list = []\n",
    "for col in cols:\n",
    "    n_list.append(collocation_list(full_data[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we find that dry clean, machine wash, 'fits true to size', 'made in italy' \n",
    "# should be collated together\n",
    "\n",
    "col_list = [['Dry', 'clean'], ['Machine', 'wash'], ['true', 'size'],['fit', 'true_size'], ['Fits', 'true_size'],  \n",
    "            ['Made', 'Italy']]\n",
    "data_col = full_data['details']\n",
    "new_list = []\n",
    "clean_list = ['Dry','wash','Italy', 'size']\n",
    "for i in range(0,len(data_col)):\n",
    "    words = word_tokenize(data_col[i])\n",
    "    len_words = len(words)-1\n",
    "    for i in range(0,len_words):\n",
    "        bi_word = []\n",
    "        j = i+1\n",
    "        bi_word.append(words[i])\n",
    "        bi_word.append(words[j])\n",
    "        if(bi_word in col_list):\n",
    "            sentence = \"_\".join(bi_word)\n",
    "            words[i] = sentence\n",
    "    for word in words:\n",
    "        if word in clean_list:\n",
    "            words.remove(word)\n",
    "    res_word = []\n",
    "    for word in words:\n",
    "        res_word.append(word)\n",
    "    sentence = \" \".join(res_word)\n",
    "    new_list.append(sentence)\n",
    "full_data['details'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we find that wide leg, ankle boot, high waist, high rise \n",
    "# should be collated together\n",
    "\n",
    "col_list = [['wide','leg'], ['ankle', 'boot'], ['High', 'Waist'], ['Wide', 'Leg'], ['High', 'Rise'], \n",
    "            ['straight', 'leg'], ['shoulder', 'bag']]\n",
    "data_col = full_data['name']\n",
    "new_list = []\n",
    "clean_list = ['leg', 'boot', 'Waist', 'Leg', 'Rise','bag']\n",
    "for i in range(0,len(data_col)):\n",
    "    words = word_tokenize(data_col[i])\n",
    "    len_words = len(words)-1\n",
    "    for i in range(0,len_words):\n",
    "        bi_word = []\n",
    "        j = i+1\n",
    "        bi_word.append(words[i])\n",
    "        bi_word.append(words[j])\n",
    "        if(bi_word in col_list):\n",
    "            sentence = \"_\".join(bi_word)\n",
    "            words[i] = sentence\n",
    "    for word in words:\n",
    "        if word in clean_list:\n",
    "            words.remove(word)\n",
    "    res_word = []\n",
    "    for word in words:\n",
    "        res_word.append(word)\n",
    "    sentence = \" \".join(res_word)\n",
    "    new_list.append(sentence)\n",
    "full_data['name'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = [['Dry','clean'], ['Machine', 'wash'], ['Made', 'Italy'], ['wide', 'leg'], ['best', 'selling'], \n",
    "            ['high', 'waisted'], ['pencil', 'skirt'], ['High', 'Rise'], ['high','rise'], ['Hand','wash']]\n",
    "data_col = full_data['description']\n",
    "new_list = []\n",
    "clean_list = ['clean', 'wash', 'Italy', 'leg', 'selling','waisted', 'skirt','Rise', 'rise']\n",
    "for i in range(0,len(data_col)):\n",
    "    words = word_tokenize(data_col[i])\n",
    "    len_words = len(words)-1\n",
    "    for i in range(0,len_words):\n",
    "        bi_word = []\n",
    "        j = i+1\n",
    "        bi_word.append(words[i])\n",
    "        bi_word.append(words[j])\n",
    "        if(bi_word in col_list):\n",
    "            sentence = \"_\".join(bi_word)\n",
    "            words[i] = sentence\n",
    "    for word in words:\n",
    "        if word in clean_list:\n",
    "            words.remove(word)\n",
    "    res_word = []\n",
    "    for word in words:\n",
    "        res_word.append(word)\n",
    "    sentence = \" \".join(res_word)\n",
    "    new_list.append(sentence)\n",
    "full_data['description'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we find that  should be collocated together\n",
    "\n",
    "col_list = [['New', 'York'], ['Jimmy','Choo'], ['Banana','Republic'], ['Victoria','Beckham']]\n",
    "data_col = data_color['brand']\n",
    "new_list = []\n",
    "clean_list = ['York', 'Choo','Republic', 'Beckham']\n",
    "for i in range(0,len(data_col)):\n",
    "    words = word_tokenize(data_col[i])\n",
    "    len_words = len(words)-1\n",
    "    for i in range(0,len_words):\n",
    "        bi_word = []\n",
    "        j = i+1\n",
    "        bi_word.append(words[i])\n",
    "        bi_word.append(words[j])\n",
    "        if(bi_word in col_list):\n",
    "            sentence = \"_\".join(bi_word)\n",
    "            words[i] = sentence\n",
    "    for word in words:\n",
    "        if word in clean_list:\n",
    "            words.remove(word)\n",
    "    res_word = []\n",
    "    for word in words:\n",
    "        res_word.append(word)\n",
    "    sentence = \" \".join(res_word)\n",
    "    new_list.append(sentence)\n",
    "data_color['brand'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = [['Mid', 'Heel'], ['Shoulder', 'Bags']]\n",
    "data_col = data_color['brand_category']\n",
    "new_list = []\n",
    "clean_list = ['Heel', 'Bags']\n",
    "for i in range(0,len(data_col)):\n",
    "    words = word_tokenize(data_col[i])\n",
    "    len_words = len(words)-1\n",
    "    for i in range(0,len_words):\n",
    "        bi_word = []\n",
    "        j = i+1\n",
    "        bi_word.append(words[i])\n",
    "        bi_word.append(words[j])\n",
    "        if(bi_word in col_list):\n",
    "            sentence = \"_\".join(bi_word)\n",
    "            words[i] = sentence\n",
    "    for word in words:\n",
    "        if word in clean_list:\n",
    "            words.remove(word)\n",
    "    res_word = []\n",
    "    for word in words:\n",
    "        res_word.append(word)\n",
    "    sentence = \" \".join(res_word)\n",
    "    new_list.append(sentence)\n",
    "data_color['brand_category'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the tf-idf vectorizer\n",
    "vectorizer = TfidfVectorizer(token_pattern=r'\\b[a-zA-Z]{3,}\\b',stop_words=\"english\",binary = True,min_df = 0.005,max_df = 0.7,max_features =300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing our test/full data\n",
    "columns = ['description','details','brand', 'brand_category', 'name']\n",
    "model_data_test=pd.DataFrame()\n",
    "for j in columns:\n",
    "    corpus = []\n",
    "    for i in range(0,len(full_data)):\n",
    "        corpus.append(full_data.loc[i,j])\n",
    "    vect = vectorizer.fit_transform(corpus)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    c=pd.DataFrame(vect.toarray().transpose(), index=terms)\n",
    "    model_data_test=pd.concat([model_data_test,c.T],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. have your pre-made df\n",
    "2. Train model on pre-made df\n",
    "2. make a df with just the 4 input columns\n",
    "3. have prof input as last row of df\n",
    "4. run all pre-processing, vectorization, etc on the df\n",
    "5. when you do train/test split, train = everything but input, test = input\n",
    "6. convert test output to string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('subset_color.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize training data\n",
    "columns = ['description','details','brand', 'brand_category', 'name']\n",
    "model_data_train=pd.DataFrame()\n",
    "for j in columns:\n",
    "    corpus = []\n",
    "    for i in range(0,len(training_data)):\n",
    "        corpus.append(training_data.loc[i,j])\n",
    "    vect = vectorizer.fit_transform(corpus)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    c=pd.DataFrame(vect.toarray().transpose(), index=terms)\n",
    "    model_data_train=pd.concat([model_data_train,c.T],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39942, 1076)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48994, 1035)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4881578288518352\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 1035 features per sample; expecting 1076",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-2ff8a7cb3e43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlogreg_black\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlogreg_black\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogreg_black\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_data_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 273\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 1035 features per sample; expecting 1076"
     ]
    }
   ],
   "source": [
    "#Train model for black\n",
    "X=model_data_train\n",
    "y=training_data['is_black'].values\n",
    "print(training_data['is_black'].sum()/len(training_data))\n",
    "logreg_black=LogisticRegression(n_jobs=1, C=1e5)      \n",
    "logreg_black.fit(X, y)\n",
    "y_pred = logreg_black.predict(model_data_test)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_class = ['is_black', 'is_beige', 'is_burgundy', 'is_white', 'is_gray', 'is_gold', 'is_blue', 'is_neutral', \n",
    "               'is_pink', 'is_orange', 'is_navy', 'is_brown', 'is_red', 'is_yellow', 'is_multi', 'is_green', \n",
    "               'is_silver', 'is_teal', 'is_purple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tag_df = pd.DataFrame(columns = color_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 1035 features per sample; expecting 1076",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-ac8f4c705817>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# have our model predict our entire test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred_black\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogreg_black\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_data_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 273\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 1035 features per sample; expecting 1076"
     ]
    }
   ],
   "source": [
    "# have our model predict our entire test data\n",
    "y_pred_black = logreg_black.predict(model_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan is to do the above code and pass them all into a dataframe, then convert data frame to color name string then append string as column onto original full_data set\n",
    "would need to attach via product id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
