{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this code before running the TF-IDF we look for collocations for different columns,add this collocations, then run the tf-idf vectorizer and then we have implemented multiple N-grams\n",
    "\n",
    "## Also, as best model in this code we are using this model to print the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import difflib\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42373, 256)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input the data columns\n",
    "data = pd.read_csv('Full+data.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis = 1,how =\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6621, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_data = pd.read_csv('extra_data.csv')\n",
    "extra_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this version we are not including brand_url\n",
    "cols = ['product_id', 'brand','description', 'brand_category', 'name','details']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6621, 6)\n",
      "(42373, 6)\n"
     ]
    }
   ],
   "source": [
    "extra_data = extra_data.loc[:,cols]\n",
    "data = data.loc[:,cols]\n",
    "print(extra_data.shape)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = pd.concat([data,extra_data])\n",
    "len(full_data) == len(data) + len(extra_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48994"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48087"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing duplicate product names\n",
    "full_data.drop_duplicates(subset=['product_id'], keep=\"first\",inplace = True)\n",
    "len(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id        0\n",
       "brand             0\n",
       "description       0\n",
       "brand_category    0\n",
       "name              0\n",
       "details           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#treating for na variables\n",
    "full_data.fillna('UNKNOWNTOKEN',inplace=True)\n",
    "full_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>brand</th>\n",
       "      <th>description</th>\n",
       "      <th>brand_category</th>\n",
       "      <th>name</th>\n",
       "      <th>details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01DSRPSZTDW2PGK1YWYXJGKZZ0</td>\n",
       "      <td>fila</td>\n",
       "      <td>vintage fitness leather sneakers with logo pri...</td>\n",
       "      <td>themensstore/shoes/sneakers/lowtop</td>\n",
       "      <td>original fitness sneakers</td>\n",
       "      <td>leather/synthetic upper\\nlace-up closure\\ntext...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01DSQXJBX0R7DCW7KTAC1SW547</td>\n",
       "      <td>chanel</td>\n",
       "      <td>unknowntoken</td>\n",
       "      <td>unknown</td>\n",
       "      <td>hat</td>\n",
       "      <td>wool tweed &amp; felt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   product_id   brand  \\\n",
       "0  01DSRPSZTDW2PGK1YWYXJGKZZ0    fila   \n",
       "1  01DSQXJBX0R7DCW7KTAC1SW547  chanel   \n",
       "\n",
       "                                         description  \\\n",
       "0  vintage fitness leather sneakers with logo pri...   \n",
       "1                                       unknowntoken   \n",
       "\n",
       "                       brand_category                       name  \\\n",
       "0  themensstore/shoes/sneakers/lowtop  original fitness sneakers   \n",
       "1                             unknown                        hat   \n",
       "\n",
       "                                             details  \n",
       "0  leather/synthetic upper\\nlace-up closure\\ntext...  \n",
       "1                                  wool tweed & felt  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clean the data for upper case\n",
    "cols = full_data.columns[1:]\n",
    "for col in cols:\n",
    "    full_data[col] = full_data[col].str.lower()\n",
    "full_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the clean tagged data \n",
    "\n",
    "tags_style_all = pd.read_csv('style.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_label = pd.merge(full_data,tags_style_all,on='product_id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_label.to_csv('data_label.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data to reduce dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first let's remove the basic stop words from the dataset\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "stop = set(STOPWORDS)\n",
    "def remove_stopwords(data_col):\n",
    "    new_list = []\n",
    "    a = data_col\n",
    "    for i in range(0,len(a)):\n",
    "        words = word_tokenize(a[i])\n",
    "        res_words = []\n",
    "        for word in words:\n",
    "            if word not in stop:\n",
    "                res_words.append(word)\n",
    "            sentence = \" \".join(res_words)\n",
    "        new_list.append(sentence)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['brand', 'description', 'brand_category', 'name','details']\n",
    "for col in cols:\n",
    "    data_label[col] = remove_stopwords(data_label[col])\n",
    "#data_label.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first let's clean description based on some rules\n",
    "#clean the data using regex\n",
    "def reg_clean(data,col):\n",
    "    new_list = []\n",
    "    for i in range(0,len(data)):\n",
    "        #special characters \n",
    "        a = re.sub(r'[^ a-zA-Z0-9]','',data.loc[i,col])\n",
    "        #remove multiple spaces by a single space\n",
    "        a = re.sub(r'\\s+',' ',a)\n",
    "        #timestamp\n",
    "        a = re.sub(r'\\b[0-9]{1,}am|[0-9]{1,}pm|[0-9]{4,}|[0-9]ish|1st|2nd|3rd|[0-9]{1,2}th|31st|[0-9]{1,}min(?:utes)?s?|[0-9]{1,}h(?:ou)?rs?|[0-9]{3,}\\b','timestamp',a)\n",
    "        a = re.sub(r'\\b[0-9]{1,}timestamp\\b','timestamp',a)\n",
    "        #any numbers as digit\n",
    "        a = re.sub(r'\\b\\d{1,}\\b','digit',a)\n",
    "        #number followed by a variable\n",
    "        a = re.sub(r'\\b\\d{1,}[a-z]{0,}[0-9]{0,}','varchar',a)\n",
    "        #html codes\n",
    "        a = re.sub(r'<.+?>','html',a)\n",
    "        a = re.sub(r'https|www','html',a)\n",
    "        new_list.append(a)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['brand', 'description', 'brand_category', 'name','details']\n",
    "for col in cols:\n",
    "    data_label[col] = reg_clean(data_label,col)\n",
    "#data_label.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize\n",
    "def lemmatize_sentence(data_col):\n",
    "    new_list = []\n",
    "    a = data_col \n",
    "    for i in range(0,len(a)):\n",
    "        words = word_tokenize(a[i])\n",
    "        res_words = []\n",
    "        for word in words:\n",
    "            res_words.append(lemmatizer.lemmatize(word).strip(string.punctuation))\n",
    "        sentence = \" \".join(res_words)\n",
    "        new_list.append(sentence)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['brand', 'description', 'brand_category', 'name','details']\n",
    "for col in cols:\n",
    "    data_label[col] = lemmatize_sentence(data_label[col])\n",
    "#data_label.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id                                   01DVME78NARMF9G6H5HTPG72Q6\n",
       "brand                                                                  \n",
       "description           offwhite satin lightbeige crepe chine partiall...\n",
       "brand_category                                       clothing top shirt\n",
       "name                                    paneled satin crepe chine shirt\n",
       "details               fit true size normal size cut loose fit midwei...\n",
       "is_casual                                                             0\n",
       "is_modern                                                             1\n",
       "is_androgynous                                                        1\n",
       "is_romantic                                                           0\n",
       "is_boho                                                               0\n",
       "is_business casual                                                    1\n",
       "is_edgy                                                               0\n",
       "is_glam                                                               0\n",
       "is_classic                                                            1\n",
       "is_athleisure                                                         0\n",
       "is_retro                                                              0\n",
       "Name: 607, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_label.loc[607,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Finding Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_coll = set(stopwords.words('english') + [\".\",'.', \",\",\":\", \"''\", \"'s\", \"'\", \"``\", \"(\", \")\", \"-\",\"timestamp\",\"varchar\",\"html\",\"digit\"])\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopwords_coll\n",
    "def collocation_list(data_col):\n",
    "    new_list = []\n",
    "    for i in range(0,len(data_col)):\n",
    "        words = word_tokenize(data_col[i])\n",
    "        res_words = []\n",
    "        for word in words:\n",
    "            if word not in stopwords_coll:\n",
    "                res_words.append(word)\n",
    "        new_list.append(res_words)\n",
    "    return(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates n-list of n size n = no of columns\n",
    "cols = ['brand', 'description', 'brand_category', 'name','details']\n",
    "n_list = []\n",
    "for col in cols:\n",
    "    n_list.append(collocation_list(data_label[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nili', 'lotan'),\n",
       " ('anine', 'bing'),\n",
       " ('tory', 'burch'),\n",
       " ('sam', 'edelman'),\n",
       " ('anthony', 'thomas'),\n",
       " ('atm', 'anthony'),\n",
       " ('thomas', 'melillo'),\n",
       " ('ulla', 'johnson'),\n",
       " ('mansur', 'gavriel'),\n",
       " ('franco', 'sarto'),\n",
       " ('citizen', 'humanity'),\n",
       " ('sarto', 'franco'),\n",
       " ('charles', 'david'),\n",
       " ('zadig', 'voltaire'),\n",
       " ('banana', 'republic'),\n",
       " ('brochu', 'walker'),\n",
       " ('veronica', 'beard'),\n",
       " ('alexandre', 'birman'),\n",
       " ('aleksandre', 'akhalkatsishvili'),\n",
       " ('isabel', 'marant'),\n",
       " ('common', 'project'),\n",
       " ('jimmy', 'choo'),\n",
       " ('jenni', 'kayne'),\n",
       " ('angela', 'scott'),\n",
       " ('gianvito', 'rossi')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain top 25 collocations by raw frequency Change the value of n from 0, length 5 for different columns\n",
    "collocation_finder = BigramCollocationFinder.from_documents(n_list[0])\n",
    "measures = BigramAssocMeasures()\n",
    "collocation_finder.apply_word_filter(filter_stops)\n",
    "collocation_finder.nbest(BigramAssocMeasures.likelihood_ratio, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we see most of these words are supposed to be used together\n",
    "#So, we collate them\n",
    "#This is collation of brands\n",
    "data_col = data_label['brand']\n",
    "new_list = []\n",
    "for i in range(0,len(data_col)):\n",
    "    words = word_tokenize(data_col[i])\n",
    "    res_word = []\n",
    "    for word in words:\n",
    "        res_word.append(word)\n",
    "    sentence = \"_\".join(res_word)\n",
    "    new_list.append(sentence)\n",
    "data_label['brand'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, we look at description\n",
    "collocation_finder = BigramCollocationFinder.from_documents(n_list[1])\n",
    "measures = BigramAssocMeasures()\n",
    "collocation_finder.apply_word_filter(filter_stops)\n",
    "#collocation_finder.nbest(BigramAssocMeasures.likelihood_ratio, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we find that dry clean, high waist, pointed toe should be collated together\n",
    "#change col_list and data_col to alter for your dataset\n",
    "col_list = [['dry', 'clean'],['high','waist'],['pointed','toe']]\n",
    "data_col = data_label['description']\n",
    "new_list = []\n",
    "clean_list = ['clean','waist','toe']\n",
    "for i in range(0,len(data_col)):\n",
    "    words = word_tokenize(data_col[i])\n",
    "    len_words = len(words)-1\n",
    "    for i in range(0,len_words):\n",
    "        bi_word = []\n",
    "        j = i+1\n",
    "        bi_word.append(words[i])\n",
    "        bi_word.append(words[j])\n",
    "        if(bi_word in col_list):\n",
    "            sentence = \"_\".join(bi_word)\n",
    "            words[i] = sentence\n",
    "    for word in words:\n",
    "        if word in clean_list:\n",
    "            words.remove(word)\n",
    "    res_word = []\n",
    "    for word in words:\n",
    "        res_word.append(word)\n",
    "    sentence = \" \".join(res_word)\n",
    "    new_list.append(sentence)\n",
    "data_label['description'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beige stretchsilk slip digit silk digit spandex dry_clean imported'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_label['description'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 'clothing'),\n",
       " ('clothing', 'top'),\n",
       " ('mid', 'heel'),\n",
       " ('boot', 'ankle'),\n",
       " ('shoe', 'pump'),\n",
       " ('shoe', 'boot'),\n",
       " ('bag', 'shoulder'),\n",
       " ('shoulder', 'bag'),\n",
       " ('clothing', 'dress'),\n",
       " ('shoe', 'sandal'),\n",
       " ('clothing', 'pant'),\n",
       " ('pump', 'mid'),\n",
       " ('top', 'blouse'),\n",
       " ('skirt', 'short'),\n",
       " ('straight', 'leg'),\n",
       " ('bag', 'tote'),\n",
       " ('medium', 'knit'),\n",
       " ('wide', 'leg'),\n",
       " ('knitwear', 'medium'),\n",
       " ('apr', 'sport'),\n",
       " ('sport', 'apr'),\n",
       " ('clothing', 'skirt'),\n",
       " ('high', 'heel'),\n",
       " ('woman', 'shoe'),\n",
       " ('tank', 'camis')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now, we look at brand_category\n",
    "collocation_finder = BigramCollocationFinder.from_documents(n_list[2])\n",
    "measures = BigramAssocMeasures()\n",
    "collocation_finder.apply_word_filter(filter_stops)\n",
    "collocation_finder.nbest(BigramAssocMeasures.likelihood_ratio, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we find that mid heel, boot ankle, shoe pump, shoe boot, shoulder bag, shoe sandal, straight leg, \n",
    "# top blouse, wide leg, high heel should be collated together\n",
    "col_list = [['mid', 'heel'],['boot','ankle'],['shoe','pump'],['shoe','boot'],['shoulder','bag'],\n",
    "            ['shoe','sandal'],['straight','leg'],['top','blouse'],['wide','leg'],['high','heel']]\n",
    "data_col = data_label['brand_category']\n",
    "new_list = []\n",
    "clean_list = ['heel','ankle','pump','bag','sandal','leg','blouse']\n",
    "for i in range(0,len(data_col)):\n",
    "    words = word_tokenize(data_col[i])\n",
    "    len_words = len(words)-1\n",
    "    for i in range(0,len_words):\n",
    "        bi_word = []\n",
    "        j = i+1\n",
    "        bi_word.append(words[i])\n",
    "        bi_word.append(words[j])\n",
    "        if(bi_word in col_list):\n",
    "            sentence = \"_\".join(bi_word)\n",
    "            words[i] = sentence\n",
    "    for word in words:\n",
    "        if word in clean_list:\n",
    "            words.remove(word)\n",
    "    res_word = []\n",
    "    for word in words:\n",
    "        res_word.append(word)\n",
    "    sentence = \" \".join(res_word)\n",
    "    new_list.append(sentence)\n",
    "data_label['brand_category'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, we look at name\n",
    "collocation_finder = BigramCollocationFinder.from_documents(n_list[3])\n",
    "measures = BigramAssocMeasures()\n",
    "collocation_finder.apply_word_filter(filter_stops)\n",
    "top25_list = collocation_finder.nbest(BigramAssocMeasures.likelihood_ratio, 25)\n",
    "top25_list.remove(('leg', 'jean'))\n",
    "top25_list.remove(('genuine', 'calf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = []\n",
    "for i in range(0,len(top25_list)):\n",
    "    a = top25_list[i][0]\n",
    "    b = top25_list[i][1]\n",
    "    col_list.append([a,b])\n",
    "clean_list = []\n",
    "for i in range(0,len(col_list)):\n",
    "    clean_list.append(col_list[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['waist',\n",
       " 'sleeve',\n",
       " 'jean',\n",
       " 'sandal',\n",
       " 'hair',\n",
       " 'dress',\n",
       " 'bag',\n",
       " 'leg',\n",
       " 'leg',\n",
       " 'max',\n",
       " 'bag',\n",
       " 'print',\n",
       " 'toe',\n",
       " 'embossed',\n",
       " 'skirt',\n",
       " 'sweater',\n",
       " 'cotton',\n",
       " 'neck',\n",
       " 'skinny',\n",
       " 'blouse',\n",
       " 'dress',\n",
       " 'strap',\n",
       " 'boot']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_col = data_label['name']\n",
    "new_list = []\n",
    "for i in range(0,len(data_col)):\n",
    "    words = word_tokenize(data_col[i])\n",
    "    len_words = len(words)-1\n",
    "    for i in range(0,len_words):\n",
    "        bi_word = []\n",
    "        j = i+1\n",
    "        bi_word.append(words[i])\n",
    "        bi_word.append(words[j])\n",
    "        if(bi_word in col_list):\n",
    "            sentence = \"_\".join(bi_word)\n",
    "            words[i] = sentence\n",
    "    for word in words:\n",
    "        if word in clean_list:\n",
    "            words.remove(word)\n",
    "    res_word = []\n",
    "    for word in words:\n",
    "        res_word.append(word)\n",
    "    sentence = \" \".join(res_word)\n",
    "    new_list.append(sentence)\n",
    "data_label['name'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, we look at name\n",
    "collocation_finder = BigramCollocationFinder.from_documents(n_list[4])\n",
    "measures = BigramAssocMeasures()\n",
    "collocation_finder.apply_word_filter(filter_stops)\n",
    "top25_list = collocation_finder.nbest(BigramAssocMeasures.likelihood_ratio, 25)\n",
    "#top25_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we find that mid heel, boot ankle, shoe pump, shoe boot, shoulder bag, shoe sandal, straight leg, \n",
    "# top blouse, wide leg, high heel should be collated together\n",
    "col_list = [['true', 'size'],['dry','clean'],['machine','wash'],['tumble','dry'],['hand','wash'],\n",
    "            ['long','sleeve'],['high','rise']]\n",
    "data_col = data_label['details']\n",
    "new_list = []\n",
    "clean_list = ['size','clean','wash','dry','sleeve','rise']\n",
    "for i in range(0,len(data_col)):\n",
    "    words = word_tokenize(data_col[i])\n",
    "    len_words = len(words)-1\n",
    "    for i in range(0,len_words):\n",
    "        bi_word = []\n",
    "        j = i+1\n",
    "        bi_word.append(words[i])\n",
    "        bi_word.append(words[j])\n",
    "        if(bi_word in col_list):\n",
    "            sentence = \"_\".join(bi_word)\n",
    "            words[i] = sentence\n",
    "    for word in words:\n",
    "        if word in clean_list:\n",
    "            words.remove(word)\n",
    "    res_word = []\n",
    "    for word in words:\n",
    "        res_word.append(word)\n",
    "    sentence = \" \".join(res_word)\n",
    "    new_list.append(sentence)\n",
    "data_label['details'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = [['true', 'size'],['dry','clean'],['machine','wash'],['tumble','dry'],['hand','wash'],\n",
    "            ['long','sleeve'],['high','rise'],['fit','true_size']]\n",
    "data_col = data_label['details']\n",
    "new_list = []\n",
    "clean_list = ['size','clean','wash','dry','sleeve','rise','true_size']\n",
    "for i in range(0,len(data_col)):\n",
    "    words = word_tokenize(data_col[i])\n",
    "    len_words = len(words)-1\n",
    "    for i in range(0,len_words):\n",
    "        bi_word = []\n",
    "        j = i+1\n",
    "        bi_word.append(words[i])\n",
    "        bi_word.append(words[j])\n",
    "        if(bi_word in col_list):\n",
    "            sentence = \"_\".join(bi_word)\n",
    "            words[i] = sentence\n",
    "    for word in words:\n",
    "        if word in clean_list:\n",
    "            words.remove(word)\n",
    "    res_word = []\n",
    "    for word in words:\n",
    "        res_word.append(word)\n",
    "    sentence = \" \".join(res_word)\n",
    "    new_list.append(sentence)\n",
    "data_label['details'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_label.to_csv('data_label.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running the above two times to add for fit_true_size which occurs a lot of times. It's a hack but works well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to vectorize our data. \n",
    "\n",
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(token_pattern=r'\\b[a-zA-Z]{3,}\\b',stop_words=\"english\",min_df = 0.005,max_df = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dimensionality of the data is:(3916, 982)\n"
     ]
    }
   ],
   "source": [
    "columns = ['brand', 'description', 'brand_category', 'name','details']\n",
    "model_data=pd.DataFrame()\n",
    "for j in columns:\n",
    "    corpus = []\n",
    "    for i in range(0,len(data_label)):\n",
    "        corpus.append(data_label.loc[i,j])\n",
    "    vect = vectorizer.fit_transform(corpus)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    c=pd.DataFrame(vect.toarray().transpose(), index=terms)\n",
    "    model_data=pd.concat([model_data,c.T],axis = 1)\n",
    "print(f'The Dimensionality of the data is:{model_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram tf-idf model\n",
    "#N = 2,3,(1,3),(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try different N values\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2),token_pattern=r'\\b[a-zA-Z]{3,}\\b',stop_words = \"english\",min_df = 1,max_df = 0.7,max_features = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dimensionality of the data is:(3916, 1064)\n"
     ]
    }
   ],
   "source": [
    "columns = ['description', 'brand_category', 'name','details']\n",
    "bi_model_data=pd.DataFrame()\n",
    "for j in columns:\n",
    "    corpus = []\n",
    "    for i in range(0,len(data_label)):\n",
    "        corpus.append(data_label.loc[i,j])\n",
    "    vect = vectorizer.fit_transform(corpus)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    c=pd.DataFrame(vect.toarray().transpose(), index=terms)\n",
    "    bi_model_data=pd.concat([bi_model_data,c.T],axis = 1)\n",
    "print(f'The Dimensionality of the data is:{bi_model_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try different N values\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3,3),token_pattern=r'\\b[a-zA-Z]{3,}\\b',stop_words=\"english\",max_df = 0.7,max_features = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dimensionality of the data is:(3916, 992)\n"
     ]
    }
   ],
   "source": [
    "columns = ['description', 'brand_category', 'name','details']\n",
    "tri_model_data=pd.DataFrame()\n",
    "for j in columns:\n",
    "    corpus = []\n",
    "    for i in range(0,len(data_label)):\n",
    "        corpus.append(data_label.loc[i,j])\n",
    "    vect = vectorizer.fit_transform(corpus)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    c=pd.DataFrame(vect.toarray().transpose(), index=terms)\n",
    "    tri_model_data=pd.concat([tri_model_data,c.T],axis = 1)\n",
    "print(f'The Dimensionality of the data is:{tri_model_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try different N values\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3),token_pattern=r'\\b[a-zA-Z]{3,}\\b',stop_words=\"english\",min_df = 10,max_df = 0.7,max_features = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dimensionality of the data is:(3916, 914)\n"
     ]
    }
   ],
   "source": [
    "columns = ['brand', 'description', 'brand_category', 'name','details']\n",
    "obi_tri_model_data=pd.DataFrame()\n",
    "for j in columns:\n",
    "    corpus = []\n",
    "    for i in range(0,len(data_label)):\n",
    "        corpus.append(data_label.loc[i,j])\n",
    "    vect = vectorizer.fit_transform(corpus)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    c=pd.DataFrame(vect.toarray().transpose(), index=terms)\n",
    "    obi_tri_model_data=pd.concat([obi_tri_model_data,c.T],axis = 1)\n",
    "print(f'The Dimensionality of the data is:{obi_tri_model_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try different N values\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,3),token_pattern=r'\\b[a-zA-Z]{3,}\\b',stop_words=stop,min_df = 10,max_df = 0.7,max_features = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dimensionality of the data is:(3916, 656)\n"
     ]
    }
   ],
   "source": [
    "columns = ['description', 'brand_category', 'name','details']\n",
    "bi_tri_model_data=pd.DataFrame()\n",
    "for j in columns:\n",
    "    corpus = []\n",
    "    for i in range(0,len(data_label)):\n",
    "        corpus.append(data_label.loc[i,j])\n",
    "    vect = vectorizer.fit_transform(corpus)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    c=pd.DataFrame(vect.toarray().transpose(), index=terms)\n",
    "    bi_tri_model_data=pd.concat([bi_tri_model_data,c.T],axis = 1)\n",
    "print(f'The Dimensionality of the data is:{bi_tri_model_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random is :79.46884576098059\n",
      "SVM Accuracy Score for n-gram (1, 1):82.78061224489795\n",
      "random is :79.46884576098059\n",
      "SVM Accuracy Score for n-gram (2, 2):81.63265306122449\n",
      "random is :79.46884576098059\n",
      "SVM Accuracy Score for n-gram (3, 3):82.2704081632653\n",
      "random is :79.46884576098059\n",
      "SVM Accuracy Score for n-gram (1, 3):85.45918367346938\n",
      "random is :79.46884576098059\n",
      "SVM Accuracy Score for n-gram (2, 3):79.97448979591837\n"
     ]
    }
   ],
   "source": [
    "#change X = bi_model_data or model_data to see how different results are changing you can do this for all the different models in other file \n",
    "# Also, use stratify = Y in all other options for train-test splitting as well\n",
    "x_list = [model_data,bi_model_data,tri_model_data,obi_tri_model_data,bi_tri_model_data]\n",
    "nval_list = [(1,1),(2,2),(3,3),(1,3),(2,3)]\n",
    "for i in range(0,len(x_list)):\n",
    "    X=x_list[i]\n",
    "    y=data_label['is_edgy'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0,stratify = y)\n",
    "    rand = data_label['is_edgy'].sum()/len(data_label)\n",
    "    print(f'random is :{max(rand,1-rand)*100}')\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(X_train,y_train)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(X_test)\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    print(f'SVM Accuracy Score for n-gram {nval_list[i]}:{accuracy_score(predictions_SVM, y_test)*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random is :67.16036772216547\n",
      "SVM Accuracy Score for n-gram (1, 1):81.50510204081633\n",
      "random is :67.16036772216547\n",
      "SVM Accuracy Score for n-gram (2, 2):76.91326530612244\n",
      "random is :67.16036772216547\n",
      "SVM Accuracy Score for n-gram (3, 3):73.72448979591837\n",
      "random is :67.16036772216547\n",
      "SVM Accuracy Score for n-gram (1, 3):79.20918367346938\n",
      "random is :67.16036772216547\n",
      "SVM Accuracy Score for n-gram (2, 3):75.63775510204081\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(x_list)):\n",
    "    X=x_list[i]\n",
    "    y=data_label['is_casual'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0,stratify = y)\n",
    "    rand = data_label['is_casual'].sum()/len(data_label)\n",
    "    print(f'random is :{max(rand,1-rand)*100}')\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(X_train,y_train)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(X_test)\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    print(f'SVM Accuracy Score for n-gram {nval_list[i]}:{accuracy_score(predictions_SVM, y_test)*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random is :53.166496424923395\n",
      "SVM Accuracy Score for n-gram (1, 1):72.95918367346938\n",
      "random is :53.166496424923395\n",
      "SVM Accuracy Score for n-gram (2, 2):68.49489795918367\n",
      "random is :53.166496424923395\n",
      "SVM Accuracy Score for n-gram (3, 3):67.7295918367347\n",
      "random is :53.166496424923395\n",
      "SVM Accuracy Score for n-gram (1, 3):74.23469387755102\n",
      "random is :53.166496424923395\n",
      "SVM Accuracy Score for n-gram (2, 3):68.23979591836735\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(x_list)):\n",
    "    X=x_list[i]\n",
    "    y=data_label['is_modern'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0,stratify = y)\n",
    "    rand = data_label['is_modern'].sum()/len(data_label)\n",
    "    print(f'random is :{max(rand,1-rand)*100}')\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(X_train,y_train)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(X_test)\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    print(f'SVM Accuracy Score for n-gram {nval_list[i]}:{accuracy_score(predictions_SVM, y_test)*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram (1,3) performs the best we need to try gridsearchcv to fine tune this parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try different N values\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3),token_pattern=r'\\b[a-zA-Z]{3,}\\b',stop_words=\"english\",min_df = 10,max_df = 0.7,max_features = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dimensionality of the data is:(3916, 1104)\n"
     ]
    }
   ],
   "source": [
    "columns = ['brand', 'description', 'brand_category', 'name','details']\n",
    "obi_tri_model_data=pd.DataFrame()\n",
    "for j in columns:\n",
    "    corpus = []\n",
    "    for i in range(0,len(data_label)):\n",
    "        corpus.append(data_label.loc[i,j])\n",
    "    vect = vectorizer.fit_transform(corpus)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    c=pd.DataFrame(vect.toarray().transpose(), index=terms)\n",
    "    obi_tri_model_data=pd.concat([obi_tri_model_data,c.T],axis = 1)\n",
    "print(f'The Dimensionality of the data is:{obi_tri_model_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best model is used to predict for the entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data_label.columns[6:17]\n",
    "for col in cols:\n",
    "    x=obi_tri_model_data\n",
    "    y=data_label[col].values\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(x,y)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(x)\n",
    "    data_label[col+'_predicted'] = predictions_SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the file to output\n",
    "data_label.to_csv('data_style_predict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
