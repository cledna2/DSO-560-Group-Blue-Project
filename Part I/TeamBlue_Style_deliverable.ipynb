{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import difflib\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp_md = spacy.load(\"en_core_web_md\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import nltk\n",
    "import logging\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We are building a function for collocation N-gram (1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### write all functions required to do so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the model_data \n",
    "data = pd.read_csv('data_label.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(STOPWORDS)\n",
    "#Function to remove stopwords\n",
    "def remove_stopwords(data,col):\n",
    "    ''' This function removes stopwords for all the columns provided as data and column as inputs'''\n",
    "    data = data\n",
    "    col = col\n",
    "    new_list = []\n",
    "    a = data[col]\n",
    "    for i in range(0,len(a)):\n",
    "        words = word_tokenize(a[i])\n",
    "        res_words = []\n",
    "        for word in words:\n",
    "            if word not in stop:\n",
    "                res_words.append(word)\n",
    "            sentence = \" \".join(res_words)\n",
    "        new_list.append(sentence)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_clean(data,col):\n",
    "    ''' This function cleans the column for a given data and column'''\n",
    "    new_list = []\n",
    "    for i in range(0,len(data)):\n",
    "        #special characters \n",
    "        a = re.sub(r'[^ a-zA-Z0-9]','',data.loc[i,col])\n",
    "        #remove multiple spaces by a single space\n",
    "        a = re.sub(r'\\s+',' ',a)\n",
    "        #timestamp\n",
    "        a = re.sub(r'\\b[0-9]{1,}am|[0-9]{1,}pm|[0-9]{4,}|[0-9]ish|1st|2nd|3rd|[0-9]{1,2}th|31st|[0-9]{1,}min(?:utes)?s?|[0-9]{1,}h(?:ou)?rs?|[0-9]{3,}\\b','timestamp',a)\n",
    "        a = re.sub(r'\\b[0-9]{1,}timestamp\\b','timestamp',a)\n",
    "        #any numbers as digit\n",
    "        a = re.sub(r'\\b\\d{1,}\\b','digit',a)\n",
    "        #number followed by a variable\n",
    "        a = re.sub(r'\\b\\d{1,}[a-z]{0,}[0-9]{0,}','varchar',a)\n",
    "        #html codes\n",
    "        a = re.sub(r'<.+?>','html',a)\n",
    "        a = re.sub(r'https|www','html',a)\n",
    "        new_list.append(a)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize\n",
    "def lemmatize_sentence(data,col):\n",
    "    ''' This function lemmatizes the column for a given data and column'''\n",
    "    new_list = []\n",
    "    a = data[col] \n",
    "    for i in range(0,len(a)):\n",
    "        words = word_tokenize(a[i])\n",
    "        res_words = []\n",
    "        for word in words:\n",
    "            res_words.append(lemmatizer.lemmatize(word).strip(string.punctuation))\n",
    "        sentence = \" \".join(res_words)\n",
    "        new_list.append(sentence)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collocation_brand(data,col='brand'):\n",
    "    ''' This is the collocation function written only for brand column'''\n",
    "    data_col = data[col]\n",
    "    new_list = []\n",
    "    for i in range(0,len(data_col)):\n",
    "        words = word_tokenize(data_col[i])\n",
    "        res_word = []\n",
    "        for word in words:\n",
    "            res_word.append(word)\n",
    "        sentence = \"_\".join(res_word)\n",
    "        new_list.append(sentence)\n",
    "    data[col] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collocation_desc(data,col='description'):\n",
    "    ''' This is the collocation function written only for description column'''\n",
    "    col_list = [['dry', 'clean'],['high','waist'],['pointed','toe']]\n",
    "    data_col = data[col]\n",
    "    new_list = []\n",
    "    clean_list = ['clean','waist','toe']\n",
    "    for i in range(0,len(data_col)):\n",
    "        words = word_tokenize(data_col[i])\n",
    "        len_words = len(words)-1\n",
    "        for i in range(0,len_words):\n",
    "            bi_word = []\n",
    "            j = i+1\n",
    "            bi_word.append(words[i])\n",
    "            bi_word.append(words[j])\n",
    "            if(bi_word in col_list):\n",
    "                sentence = \"_\".join(bi_word)\n",
    "                words[i] = sentence\n",
    "        for word in words:\n",
    "            if word in clean_list:\n",
    "                words.remove(word)\n",
    "        res_word = []\n",
    "        for word in words:\n",
    "            res_word.append(word)\n",
    "        sentence = \" \".join(res_word)\n",
    "        new_list.append(sentence)\n",
    "    data[col] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collocation_brandcat(data,col='brand_category'):\n",
    "    ''' This is a collacation function written just for brand category'''\n",
    "    col_list = [['mid', 'heel'],['boot','ankle'],['shoe','pump'],['shoe','boot'],['shoulder','bag'],\n",
    "                ['shoe','sandal'],['straight','leg'],['top','blouse'],['wide','leg'],['high','heel']]\n",
    "    data_col = data[col]\n",
    "    new_list = []\n",
    "    clean_list = ['heel','ankle','pump','bag','sandal','leg','blouse']\n",
    "    for i in range(0,len(data_col)):\n",
    "        words = word_tokenize(data_col[i])\n",
    "        len_words = len(words)-1\n",
    "        for i in range(0,len_words):\n",
    "            bi_word = []\n",
    "            j = i+1\n",
    "            bi_word.append(words[i])\n",
    "            bi_word.append(words[j])\n",
    "            if(bi_word in col_list):\n",
    "                sentence = \"_\".join(bi_word)\n",
    "                words[i] = sentence\n",
    "        for word in words:\n",
    "            if word in clean_list:\n",
    "                words.remove(word)\n",
    "        res_word = []\n",
    "        for word in words:\n",
    "            res_word.append(word)\n",
    "        sentence = \" \".join(res_word)\n",
    "        new_list.append(sentence)\n",
    "    data[col] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collocation_name(data,col='name'):\n",
    "    '''This collocation function is written just for name''' \n",
    "    col_list = [['high', 'waist'],['long', 'sleeve'],['skinny', 'jean'],['slide', 'sandal'],['calf', 'hair'],\n",
    "                ['midi', 'dress'],['crossbody', 'bag'],['straight', 'leg'],['wide', 'leg'],['air', 'max'],\n",
    "                ['shoulder', 'bag'],['leopard', 'print'],['pointy', 'toe'],['croc', 'embossed'],['midi', 'skirt'],\n",
    "                ['cashmere', 'sweater'],['pima', 'cotton'],['mock', 'neck'],['ankle', 'skinny'],['silk', 'blouse'],\n",
    "                ['maxi', 'dress'],['ankle', 'strap'],['ankle', 'boot']]\n",
    "    clean_list = ['waist','sleeve','jean','sandal','hair','dress','bag','leg','max','bag','print','toe',\n",
    "                  'embossed','skirt','sweater','cotton','neck','skinny','blouse','dress','strap','boot']\n",
    "    data_col = data[col]\n",
    "    new_list = []\n",
    "    for i in range(0,len(data_col)):\n",
    "        words = word_tokenize(data_col[i])\n",
    "        len_words = len(words)-1\n",
    "        for i in range(0,len_words):\n",
    "            bi_word = []\n",
    "            j = i+1\n",
    "            bi_word.append(words[i])\n",
    "            bi_word.append(words[j])\n",
    "            if(bi_word in col_list):\n",
    "                sentence = \"_\".join(bi_word)\n",
    "                words[i] = sentence\n",
    "        for word in words:\n",
    "            if word in clean_list:\n",
    "                words.remove(word)\n",
    "        res_word = []\n",
    "        for word in words:\n",
    "            res_word.append(word)\n",
    "        sentence = \" \".join(res_word)\n",
    "        new_list.append(sentence)\n",
    "    data[col] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collocation_details(data,col='details'):\n",
    "    ''' This collocationfunction is written just for details column'''\n",
    "    col_list = [['true', 'size'],['dry','clean'],['machine','wash'],['tumble','dry'],['hand','wash'],\n",
    "                ['long','sleeve'],['high','rise']]\n",
    "    data_col = data[col]\n",
    "    new_list = []\n",
    "    clean_list = ['size','clean','wash','dry','sleeve','rise']\n",
    "    for i in range(0,len(data_col)):\n",
    "        words = word_tokenize(data_col[i])\n",
    "        len_words = len(words)-1\n",
    "        for i in range(0,len_words):\n",
    "            bi_word = []\n",
    "            j = i+1\n",
    "            bi_word.append(words[i])\n",
    "            bi_word.append(words[j])\n",
    "            if(bi_word in col_list):\n",
    "                sentence = \"_\".join(bi_word)\n",
    "                words[i] = sentence\n",
    "        for word in words:\n",
    "            if word in clean_list:\n",
    "                words.remove(word)\n",
    "        res_word = []\n",
    "        for word in words:\n",
    "            res_word.append(word)\n",
    "        sentence = \" \".join(res_word)\n",
    "        new_list.append(sentence)\n",
    "    data[col] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the last row to test for the function\n",
    "\n",
    "data_label = data.loc[:3913,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating fake data to test the function\n",
    "brand = data.loc[3914:3915,'brand']\n",
    "description = data.loc[3914:3915,'description']\n",
    "brand_cat = data.loc[3914:3915,'brand_category']\n",
    "name = data.loc[3914:3915,'name']\n",
    "details = data.loc[3914:3915,'details']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3),token_pattern=r'\\b[a-zA-Z]{3,}\\b',stop_words=\"english\",min_df = 10,max_df = 0.7,max_features = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best function parameters \n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definig the function to predict for new input\n",
    "def predict_style(brand,description,brand_cat,name,details,train_data = data_label):\n",
    "    ''' This function has 6 inputs train_data with default input as data_label '''\n",
    "    ''' Brand, description,brand_cat,name, and details have to be fed using a series'''\n",
    "    len_ref = len(train_data)\n",
    "    data = train_data[['brand','description','brand_category','name','details']]\n",
    "    data_x = pd.DataFrame()\n",
    "    print(len(data))\n",
    "    data_x['brand'] = data['brand'].append(brand)\n",
    "    data_x['description'] = data['description'].append(description)\n",
    "    data_x['brand_category'] = data['brand_category'].append(brand_cat)\n",
    "    data_x['name'] = data['name'].append(name)\n",
    "    data_x['details'] = data['details'].append(details)\n",
    "    data_x.reset_index(drop = True)\n",
    "    print(len(data_x))\n",
    "    print(data_x.shape)\n",
    "    \n",
    "    #First let's remove stopwords\n",
    "    cols = data_x.columns\n",
    "    for col in cols:\n",
    "        remove_stopwords(data_x,col)\n",
    "        \n",
    "    #next, clean the data using regex \n",
    "    cols = data_x.columns\n",
    "    for col in cols:\n",
    "        data_label[col] = reg_clean(data_label,col)\n",
    "    \n",
    "    #next,lemmatize the data\n",
    "    cols = data_x.columns\n",
    "    for col in cols:\n",
    "        data_label[col] = lemmatize_sentence(data_label,col)\n",
    "    #then, run collocations of the data \n",
    "    collocation_brand(data_x)\n",
    "    collocation_desc(data_x)\n",
    "    collocation_brandcat(data_x)\n",
    "    collocation_name(data_x)\n",
    "    collocation_details(data_x)\n",
    "    #Vectorize the entir corpus\n",
    "    vect_data = pd.DataFrame()\n",
    "    cols = data_x.columns\n",
    "    for j in cols:\n",
    "        corpus = []\n",
    "        for i in range(0,len(data_x)):\n",
    "            corpus.append(data_x.loc[i,j])\n",
    "        vect = vectorizer.fit_transform(corpus)\n",
    "        terms = vectorizer.get_feature_names()\n",
    "        c=pd.DataFrame(vect.toarray().transpose(), index=terms)\n",
    "        vect_data=pd.concat([vect_data,c.T],axis = 1)\n",
    "    print(f'The Dimensionality of the vectorized data is:{vect_data.shape}')\n",
    "    #now, let's build the model on length data\n",
    "    X_train = vect_data[:len_ref]\n",
    "    X_test = vect_data[len_ref:]\n",
    "    #for is_casual\n",
    "    Y_train = train_data['is_casual']\n",
    "    SVM.fit(X_train,Y_train)\n",
    "    predictions_SVM = SVM.predict(X_test)\n",
    "    for i in range(0,len(predictions_SVM)):\n",
    "        if predictions_SVM[i]:\n",
    "            print(f'The product in document {i} \"is casual\"')\n",
    "        else:\n",
    "            print(f'The product in document {i} \"is not casual\"')    \n",
    "    #for is_modern\n",
    "    Y_train = train_data['is_modern']\n",
    "    SVM.fit(X_train,Y_train)\n",
    "    predictions_SVM = SVM.predict(X_test)\n",
    "    for i in range(0,len(predictions_SVM)):\n",
    "        if predictions_SVM[i]:\n",
    "            print(f'The product in document {i} \"is modern\"')\n",
    "        else:\n",
    "            print(f'The product in document {i} \"is not modern\"')\n",
    "    #for is_edgy\n",
    "    Y_train = train_data['is_edgy']\n",
    "    SVM.fit(X_train,Y_train)\n",
    "    predictions_SVM = SVM.predict(X_test)\n",
    "    for i in range(0,len(predictions_SVM)):\n",
    "        if predictions_SVM[i]:\n",
    "            print(f'The product in document {i} \"is edgy\"')\n",
    "        else:\n",
    "            print(f'The product in document {i} \"is not edgy\"')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3914\n",
      "3916\n",
      "(3916, 5)\n",
      "The Dimensionality of the vectorized data is:(3916, 1016)\n",
      "The product in document 0 \"is not casual\"\n",
      "The product in document 1 \"is casual\"\n",
      "The product in document 0 \"is not modern\"\n",
      "The product in document 1 \"is not modern\"\n",
      "The product in document 0 \"is edgy\"\n",
      "The product in document 1 \"is not edgy\"\n"
     ]
    }
   ],
   "source": [
    "#calling the functions\n",
    "predict_style(brand,description,brand_cat,name,details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating excel file for entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First let's remove stopwords\n",
    "cols = data_x.columns\n",
    "for col in cols:\n",
    "    remove_stopwords(data_x,col)\n",
    "        \n",
    "    #next, clean the data using regex \n",
    "    cols = data_x.columns\n",
    "    for col in cols:\n",
    "        data_label[col] = reg_clean(data_label,col)\n",
    "    \n",
    "    #next,lemmatize the data\n",
    "    cols = data_x.columns\n",
    "    for col in cols:\n",
    "        data_label[col] = lemmatize_sentence(data_label,col)\n",
    "    #then, run collocations of the data \n",
    "    collocation_brand(data_x)\n",
    "    collocation_desc(data_x)\n",
    "    collocation_brandcat(data_x)\n",
    "    collocation_name(data_x)\n",
    "    collocation_details(data_x)\n",
    "    #Vectorize the entir corpus\n",
    "    vect_data = pd.DataFrame()\n",
    "    cols = data_x.columns\n",
    "    for j in cols:\n",
    "        corpus = []\n",
    "        for i in range(0,len(data_x)):\n",
    "            corpus.append(data_x.loc[i,j])\n",
    "        vect = vectorizer.fit_transform(corpus)\n",
    "        terms = vectorizer.get_feature_names()\n",
    "        c=pd.DataFrame(vect.toarray().transpose(), index=terms)\n",
    "        vect_data=pd.concat([vect_data,c.T],axis = 1)\n",
    "    print(f'The Dimensionality of the vectorized data is:{vect_data.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
