{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import logging\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the cleaned data set with attribute name-category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=pd.read_csv('c.csv')\n",
    "y=c['attribute_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            top\n",
       "1            top\n",
       "2       onepiece\n",
       "3       onepiece\n",
       "4         bottom\n",
       "          ...   \n",
       "6775      bottom\n",
       "6776      bottom\n",
       "6777      bottom\n",
       "6778      bottom\n",
       "6779         top\n",
       "Name: attribute_value, Length: 6780, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['attribute_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat=c[['product_id','description','details','name','brand_category']].reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=c['attribute_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cat(brand,description,brand_category,wv):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    c=pd.read_csv('c.csv')\n",
    "    df_cat=c[['product_id','description','details','name','brand_category']].reset_index().drop(columns='index')\n",
    "    y=c['attribute_value']\n",
    "    #Clean the query\n",
    "    def replace(word):\n",
    "        word=word.replace('/',' ')\n",
    "        new=re.sub('[^a-zA-Z]', ' ',word)\n",
    "        p=[]\n",
    "        for k in new.split():\n",
    "            separated=re.sub(r'([a-z](?=[A-Z])|[A-Z](?=[A-Z][a-z]))', r'\\1 ',k)\n",
    "            new = separated.lower()\n",
    "            p.append(new)\n",
    "        ls=WordNetLemmatizer()\n",
    "        new = [ls.lemmatize(word) for word in p if not word in set(stopwords.words('english'))]\n",
    "        new = ' '.join(new)\n",
    "        return new\n",
    "    brand=replace(brand)\n",
    "    description=replace(description)\n",
    "    brand_category=replace(brand_category)\n",
    "    import gensim\n",
    "    from gensim.models import Word2Vec\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    #wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "    wv.init_sims(replace=True)\n",
    "    def word_averaging(wv, words):\n",
    "        all_words, mean = set(), []\n",
    "\n",
    "        for word in words:\n",
    "            if isinstance(word, np.ndarray):\n",
    "                mean.append(word)\n",
    "            elif word in wv.vocab:\n",
    "                mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "                all_words.add(wv.vocab[word].index)\n",
    "\n",
    "        if not mean:\n",
    "            logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "            # FIXME: remove these examples in pre-processing\n",
    "            return np.zeros(wv.vector_size,)\n",
    "\n",
    "        mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "        return mean\n",
    "\n",
    "    def  word_averaging_list(wv, text_list):\n",
    "        return np.vstack([word_averaging(wv, post) for post in text_list ])\n",
    "    def w2v_tokenize_text(text):\n",
    "        tokens = []\n",
    "        for sent in nltk.sent_tokenize(text, language='english'):\n",
    "            for word in nltk.word_tokenize(sent, language='english'):\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                tokens.append(word)\n",
    "        return tokens\n",
    "    names=['description','brand_category','details']\n",
    "    d=pd.DataFrame()\n",
    "    \n",
    "    brand=w2v_tokenize_text(brand)\n",
    "    description=w2v_tokenize_text(description)\n",
    "    brand_category=w2v_tokenize_text(brand_category)\n",
    "    \n",
    "    brand=word_averaging(wv,brand).reshape(1,-1)\n",
    "    description=word_averaging(wv,description).reshape(1,-1)\n",
    "    brand_category=word_averaging(wv,brand_category).reshape(1,-1)\n",
    "    X4=np.concatenate((brand,description,brand_category),axis=1)\n",
    "\n",
    "    for i in names:\n",
    "        X_new = df_cat.apply(lambda r: w2v_tokenize_text(r[i]),axis=1).values\n",
    "        X_word_average = word_averaging_list(wv,X_new)\n",
    "        X_array=pd.DataFrame(X_word_average)\n",
    "        d=pd.concat([d,X_array],axis=1)\n",
    "\n",
    "    nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100,50)).fit(d, y)\n",
    "    return nn.predict(X4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc=\"blush linen button fastenings along front 100% linen, lining: 100%, cotton dry clean designer color: shell imported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand='Zimmermann'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_cat='clothing/jumpsuit/full length'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:cannot compute similarity with no input ['zimmermann']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['onepiece'], dtype='<U19')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_cat(desc,brand,brand_cat,wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q=brand+' '+brand_cat+' '+desc\n",
    "X2=w2v_tokenize_text(brand)\n",
    "X3=word_averaging_list(wv,X2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
