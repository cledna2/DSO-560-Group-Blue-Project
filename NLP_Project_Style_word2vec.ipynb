{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "nlp = en_core_web_sm.load()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "nlp_md = spacy.load(\"en_core_web_md\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import nltk\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3916, 17)\n",
      "                   product_id           brand  \\\n",
      "0  01DTJCERF6F4NRZ2WSJFFA1EYS          theory   \n",
      "1  01DVPBJ6464YKYGVAE0A1HMKGN  alexander_wang   \n",
      "\n",
      "                                         description           brand_category  \\\n",
      "0  beige stretchsilk slip digit silk digit spande...  clothing top tank camis   \n",
      "1  black velvet concealed hook zip fastening digi...      clothing dress mini   \n",
      "\n",
      "                        name  \\\n",
      "0  teah stretchsilk camisole   \n",
      "1        layered velvet mini   \n",
      "\n",
      "                                             details  is_casual  is_modern  \\\n",
      "0  fit_true_size normal cut slightly loose fit li...        1.0        1.0   \n",
      "1  fit_true_size normal designed fitted bust wais...        0.0        1.0   \n",
      "\n",
      "   is_androgynous  is_romantic  is_boho  is_business casual  is_edgy  is_glam  \\\n",
      "0             0.0          1.0      0.0                 1.0      0.0      1.0   \n",
      "1             0.0          0.0      0.0                 0.0      0.0      1.0   \n",
      "\n",
      "   is_classic  is_athleisure  is_retro  \n",
      "0         1.0            0.0       0.0  \n",
      "1         1.0            0.0       0.0  \n"
     ]
    }
   ],
   "source": [
    "#input the data\n",
    "data_label = pd.read_csv('data_label.csv',index_col = 0)\n",
    "print(data_label.shape)\n",
    "print(data_label.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id              0\n",
       "brand                   1\n",
       "description             0\n",
       "brand_category        190\n",
       "name                    0\n",
       "details               224\n",
       "is_casual               0\n",
       "is_modern               0\n",
       "is_androgynous          0\n",
       "is_romantic             0\n",
       "is_boho                 0\n",
       "is_business casual      0\n",
       "is_edgy                 0\n",
       "is_glam                 0\n",
       "is_classic              0\n",
       "is_athleisure           0\n",
       "is_retro                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This nulls are incoprated due to cleaning process in previous code\n",
    "data_label.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id            0\n",
       "brand                 0\n",
       "description           0\n",
       "brand_category        0\n",
       "name                  0\n",
       "details               0\n",
       "is_casual             0\n",
       "is_modern             0\n",
       "is_androgynous        0\n",
       "is_romantic           0\n",
       "is_boho               0\n",
       "is_business casual    0\n",
       "is_edgy               0\n",
       "is_glam               0\n",
       "is_classic            0\n",
       "is_athleisure         0\n",
       "is_retro              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_label.fillna('unknowntoken',inplace=True)\n",
    "data_label.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained word2vec using en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['brand', 'description', 'brand_category', 'name','details']\n",
    "data_label_sub = data_label.loc[:,columns]\n",
    "w2v_model_sm=pd.DataFrame()\n",
    "for col in columns:\n",
    "    new_list = []\n",
    "    for i in range(0,len(data_label_sub)):\n",
    "        new_list.append(nlp(data_label_sub.loc[i,col]).vector)\n",
    "    X_array=pd.DataFrame(new_list)\n",
    "    w2v_model_sm=pd.concat([w2v_model_sm,X_array],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.495816</td>\n",
       "      <td>1.007125</td>\n",
       "      <td>0.592011</td>\n",
       "      <td>-0.989226</td>\n",
       "      <td>1.161886</td>\n",
       "      <td>1.720696</td>\n",
       "      <td>6.638313</td>\n",
       "      <td>-0.599922</td>\n",
       "      <td>0.952174</td>\n",
       "      <td>2.253995</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089093</td>\n",
       "      <td>-1.037553</td>\n",
       "      <td>0.481995</td>\n",
       "      <td>0.864222</td>\n",
       "      <td>1.041488</td>\n",
       "      <td>-0.710707</td>\n",
       "      <td>-0.322838</td>\n",
       "      <td>1.966523</td>\n",
       "      <td>1.461966</td>\n",
       "      <td>0.629186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.723979</td>\n",
       "      <td>-4.703322</td>\n",
       "      <td>0.231796</td>\n",
       "      <td>0.710519</td>\n",
       "      <td>3.448232</td>\n",
       "      <td>1.050524</td>\n",
       "      <td>1.041941</td>\n",
       "      <td>1.054857</td>\n",
       "      <td>1.878826</td>\n",
       "      <td>0.328565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402010</td>\n",
       "      <td>-1.576209</td>\n",
       "      <td>0.184507</td>\n",
       "      <td>1.262392</td>\n",
       "      <td>1.111494</td>\n",
       "      <td>0.486093</td>\n",
       "      <td>-0.532086</td>\n",
       "      <td>1.550060</td>\n",
       "      <td>1.258919</td>\n",
       "      <td>0.871626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 480 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.495816  1.007125  0.592011 -0.989226  1.161886  1.720696  6.638313   \n",
       "1  0.723979 -4.703322  0.231796  0.710519  3.448232  1.050524  1.041941   \n",
       "\n",
       "         7         8         9   ...        86        87        88        89  \\\n",
       "0 -0.599922  0.952174  2.253995  ... -0.089093 -1.037553  0.481995  0.864222   \n",
       "1  1.054857  1.878826  0.328565  ...  0.402010 -1.576209  0.184507  1.262392   \n",
       "\n",
       "         90        91        92        93        94        95  \n",
       "0  1.041488 -0.710707 -0.322838  1.966523  1.461966  0.629186  \n",
       "1  1.111494  0.486093 -0.532086  1.550060  1.258919  0.871626  \n",
       "\n",
       "[2 rows x 480 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_sm.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained word2vec using en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['brand', 'description', 'brand_category', 'name','details']\n",
    "data_label_sub = data_label.loc[:,columns]\n",
    "w2v_model_md=pd.DataFrame()\n",
    "for col in columns:\n",
    "    new_list = []\n",
    "    for i in range(0,len(data_label_sub)):\n",
    "        new_list.append(nlp_md(data_label_sub.loc[i,col]).vector)\n",
    "    X_array=pd.DataFrame(new_list)\n",
    "    w2v_model_md=pd.concat([w2v_model_md,X_array],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3916, 1500)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_md.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.26227</td>\n",
       "      <td>0.14685</td>\n",
       "      <td>-0.31801</td>\n",
       "      <td>-0.15813</td>\n",
       "      <td>-0.66168</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>-0.21232</td>\n",
       "      <td>0.29958</td>\n",
       "      <td>0.3585</td>\n",
       "      <td>2.0177</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076138</td>\n",
       "      <td>0.053004</td>\n",
       "      <td>0.157833</td>\n",
       "      <td>0.082269</td>\n",
       "      <td>-0.164426</td>\n",
       "      <td>-0.032932</td>\n",
       "      <td>-0.083052</td>\n",
       "      <td>-0.044259</td>\n",
       "      <td>0.148788</td>\n",
       "      <td>-0.027553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017492</td>\n",
       "      <td>0.021150</td>\n",
       "      <td>0.054696</td>\n",
       "      <td>0.097320</td>\n",
       "      <td>-0.116556</td>\n",
       "      <td>-0.080968</td>\n",
       "      <td>-0.054529</td>\n",
       "      <td>0.084784</td>\n",
       "      <td>0.139407</td>\n",
       "      <td>-0.041822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 1500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0        1        2        3        4         5        6        7    \\\n",
       "0 -0.26227  0.14685 -0.31801 -0.15813 -0.66168  0.057471 -0.21232  0.29958   \n",
       "1  0.00000  0.00000  0.00000  0.00000  0.00000  0.000000  0.00000  0.00000   \n",
       "\n",
       "      8       9    ...       290       291       292       293       294  \\\n",
       "0  0.3585  2.0177  ... -0.076138  0.053004  0.157833  0.082269 -0.164426   \n",
       "1  0.0000  0.0000  ... -0.017492  0.021150  0.054696  0.097320 -0.116556   \n",
       "\n",
       "        295       296       297       298       299  \n",
       "0 -0.032932 -0.083052 -0.044259  0.148788 -0.027553  \n",
       "1 -0.080968 -0.054529  0.084784  0.139407 -0.041822  \n",
       "\n",
       "[2 rows x 1500 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_md.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-idf to weight differnt pre-trained words from these list of word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dimensionality of the data is:(3916, 11477)\n"
     ]
    }
   ],
   "source": [
    "columns = ['brand', 'description', 'brand_category', 'name','details']\n",
    "tf_idf_data=pd.DataFrame()\n",
    "for j in columns:\n",
    "    corpus = []\n",
    "    for i in range(0,len(data_label)):\n",
    "        corpus.append(data_label.loc[i,j])\n",
    "    vect = vectorizer.fit_transform(corpus)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    c=pd.DataFrame(vect.toarray().transpose(), index=terms)\n",
    "    tf_idf_data=pd.concat([tf_idf_data,c.T],axis = 1)\n",
    "print(f'The Dimensionality of the data is:{tf_idf_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_SUM_COLUMN = \"DOCUMENT_TF_IDF_SUM\"\n",
    "\n",
    "# sum the tf idf scores for each document\n",
    "tf_idf_data[DOCUMENT_SUM_COLUMN] = tf_idf_data.sum(axis=1)\n",
    "available_tf_idf_scores = tf_idf_data.columns # a list of all the columns we have\n",
    "available_tf_idf_scores = list(map( lambda x: x.lower(), available_tf_idf_scores)) # lowercase everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_md(data_label_sub.loc[1,'brand']).text.lower() in available_tf_idf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['brand','description','brand_category', 'name','details']\n",
    "data_label_sub = data_label.loc[:,columns]\n",
    "w2v_md_tfidf_model=pd.DataFrame()\n",
    "for col in columns:\n",
    "    new_list = []\n",
    "    for i in range(0,len(data_label_sub)):\n",
    "        tokens = nlp_md(data_label_sub.loc[i,col])\n",
    "        # initially start a running total of tf-idf scores for a document\n",
    "        total_tf_idf_score_per_document = 0\n",
    "        # start a running total of initially all zeroes (300 is picked since that is the word embedding size used by word2vec)\n",
    "        running_total_word_embedding = np.zeros(300) \n",
    "        for token in tokens: # iterate through each token\n",
    "        # if the token has a pretrained word embedding it also has a tf-idf score\n",
    "            if token.has_vector and token.text.lower() in available_tf_idf_scores:\n",
    "                tf_idf_score = tf_idf_data.loc[i, token.text.lower()]\n",
    "                tf_idf_score = tf_idf_score.mean() #there could be multiple tags\n",
    "                #print(f\"{token} has tf-idf score of {tf_idf_score}\")\n",
    "                #print(f\"{token.vector} is the vector score\")\n",
    "                running_total_word_embedding += tf_idf_score * token.vector\n",
    "                total_tf_idf_score_per_document += tf_idf_score\n",
    "        # divide the total embedding by the total tf-idf score for each document\n",
    "        if (running_total_word_embedding.sum() == 0):\n",
    "            document_embedding = running_total_word_embedding\n",
    "        else:\n",
    "            document_embedding = running_total_word_embedding / total_tf_idf_score_per_document\n",
    "        new_list.append(document_embedding)\n",
    "    X_array=pd.DataFrame(new_list)\n",
    "    w2v_md_tfidf_model=pd.concat([w2v_md_tfidf_model,X_array],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3916, 1500)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_md_tfidf_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp(data_label.loc[0,'brand']).vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['brand','description','brand_category', 'name','details']\n",
    "data_label_sub = data_label.loc[:,columns]\n",
    "w2v_sm_tfidf_model=pd.DataFrame()\n",
    "for col in columns:\n",
    "    new_list = []\n",
    "    for i in range(0,len(data_label_sub)):\n",
    "        tokens = nlp(data_label_sub.loc[i,col])\n",
    "        # initially start a running total of tf-idf scores for a document\n",
    "        total_tf_idf_score_per_document = 0\n",
    "        # start a running total of initially all zeroes (96 is picked since that is the word embedding size used by word2vec em)\n",
    "        running_total_word_embedding = np.zeros(96) \n",
    "        for token in tokens: # iterate through each token\n",
    "        # if the token has a pretrained word embedding it also has a tf-idf score\n",
    "            if token.has_vector and token.text.lower() in available_tf_idf_scores:\n",
    "                tf_idf_score = tf_idf_data.loc[i, token.text.lower()]\n",
    "                tf_idf_score = tf_idf_score.mean() #there could be multiple tags\n",
    "                #print(f\"{token} has tf-idf score of {tf_idf_score}\")\n",
    "                #print(f\"{token.vector} is the vector score\")\n",
    "                running_total_word_embedding += tf_idf_score * token.vector\n",
    "                total_tf_idf_score_per_document += tf_idf_score\n",
    "        # divide the total embedding by the total tf-idf score for each document\n",
    "        if (running_total_word_embedding.sum() == 0):\n",
    "            document_embedding = running_total_word_embedding\n",
    "        else:\n",
    "            document_embedding = running_total_word_embedding / total_tf_idf_score_per_document\n",
    "        new_list.append(document_embedding)\n",
    "    X_array=pd.DataFrame(new_list)\n",
    "    w2v_sm_tfidf_model=pd.concat([w2v_sm_tfidf_model,X_array],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3916, 480)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_sm_tfidf_model.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As, we can see there is an improvement over training averaging word embeddings using tf-idf but there is still a lot to be desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We next implement word2vec using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "#list(islice(wv.vocab, 13030, 13050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        #logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3916, 1500)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['brand','description','brand_category', 'name','details']\n",
    "word2vec_genism_model =pd.DataFrame()\n",
    "for i in columns:\n",
    "    X_new = data_label.apply(lambda r: w2v_tokenize_text(r[i]),axis=1).values\n",
    "    X_word_average = word_averaging_list(wv,X_new)\n",
    "    X_array=pd.DataFrame(X_word_average)\n",
    "    word2vec_genism_model=pd.concat([word2vec_genism_model,X_array],axis=1)\n",
    "word2vec_genism_model.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2_vec implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['brand','description','brand_category', 'name','details']\n",
    "data_label_sub = data_label.loc[:,columns]\n",
    "data_label_sub['X'] = data_label_sub['brand']+' '+data_label_sub['description']+' '+data_label_sub['brand_category']+' '+data_label_sub['name']+' '+data_label_sub['details']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(data_label_sub['X'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(documents, vector_size=250, window=6, min_count=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['brand','description','brand_category', 'name','details']\n",
    "data_label_sub = data_label.loc[:,columns]\n",
    "doc2vec_gen_model = pd.DataFrame()\n",
    "for col in columns:\n",
    "    new_list = []\n",
    "    for i in range(0,len(data_label_sub)):\n",
    "        new_list.append(model.infer_vector([data_label_sub.loc[i,col]]))\n",
    "    X_array=pd.DataFrame(new_list)\n",
    "    doc2vec_gen_model=pd.concat([doc2vec_gen_model,X_array],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3916, 1250)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_gen_model.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random for is_modern is :53.166496424923395\n",
      "SVM Accuracy Score for model word2_vec_sm for columns is_modern is:73.08673469387756\n",
      "random for is_modern is :53.166496424923395\n",
      "SVM Accuracy Score for model w2v_sm_tfidf_model for columns is_modern is:73.34183673469387\n",
      "random for is_modern is :53.166496424923395\n",
      "SVM Accuracy Score for model word2_vec_md for columns is_modern is:71.5561224489796\n",
      "random for is_modern is :53.166496424923395\n",
      "SVM Accuracy Score for model w2v_md_tfidf_model for columns is_modern is:71.93877551020408\n",
      "random for is_modern is :53.166496424923395\n",
      "SVM Accuracy Score for model word2vec_genism_model for columns is_modern is:53.18877551020408\n",
      "random for is_modern is :53.166496424923395\n",
      "SVM Accuracy Score for model doc2vec_gen_model for columns is_modern is:53.18877551020408\n",
      "random for is_androgynous is :82.40551583248212\n",
      "SVM Accuracy Score for model word2_vec_sm for columns is_androgynous is:85.20408163265306\n",
      "random for is_androgynous is :82.40551583248212\n",
      "SVM Accuracy Score for model w2v_sm_tfidf_model for columns is_androgynous is:85.33163265306123\n",
      "random for is_androgynous is :82.40551583248212\n",
      "SVM Accuracy Score for model word2_vec_md for columns is_androgynous is:82.39795918367348\n",
      "random for is_androgynous is :82.40551583248212\n",
      "SVM Accuracy Score for model w2v_md_tfidf_model for columns is_androgynous is:82.39795918367348\n",
      "random for is_androgynous is :82.40551583248212\n",
      "SVM Accuracy Score for model word2vec_genism_model for columns is_androgynous is:82.39795918367348\n",
      "random for is_androgynous is :82.40551583248212\n",
      "SVM Accuracy Score for model doc2vec_gen_model for columns is_androgynous is:82.39795918367348\n"
     ]
    }
   ],
   "source": [
    "x_list = [w2v_model_sm,w2v_sm_tfidf_model,w2v_model_md,w2v_md_tfidf_model,word2vec_genism_model,doc2vec_gen_model]\n",
    "name = ['word2_vec_sm','w2v_sm_tfidf_model','word2_vec_md','w2v_md_tfidf_model','word2vec_genism_model','doc2vec_gen_model']\n",
    "cols = data_label.columns[7:9]\n",
    "for col in cols:\n",
    "    for i in range(0,len(x_list)):\n",
    "        X=x_list[i]    \n",
    "        y=data_label[col].values\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0,stratify = y)\n",
    "        rand = data_label[col].sum()/len(data_label)\n",
    "        print(f'random for {col} is :{max(rand,1-rand)*100}')\n",
    "        SVM = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto')\n",
    "        SVM.fit(X_train,y_train)\n",
    "        # predict the labels on validation dataset\n",
    "        predictions_SVM = SVM.predict(X_test)\n",
    "        # Use accuracy_score function to get the accuracy\n",
    "        print(f'SVM Accuracy Score for model {name[i]} for columns {col} is:{accuracy_score(predictions_SVM, y_test)*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Keras to build a classification model - LSTM, RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['brand','description','brand_category', 'name','details']\n",
    "data_label_sub = data_label.loc[:,columns]\n",
    "docs = data_label_sub['brand']+' '+data_label_sub['description']+' '+data_label_sub['brand_category']+' '+data_label_sub['name']+' '+data_label_sub['details']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "stopwords_removed_docs = list(\n",
    "    map(lambda doc: \" \".join([token.text for token in nlp(doc) if not token.is_stop]), docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords_removed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"unknowntoken\")\n",
    "tokenizer.fit_on_texts(stopwords_removed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_token_length_per_doc(docs: List[List[str]])-> int:\n",
    "    return max(list(map(lambda x: len(x.split()), docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH= get_max_token_length_per_doc(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_encode_documents(docs, tokenizer):\n",
    "    return tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "# integer encode the documents\n",
    "encoded_docs = integer_encode_documents(stopwords_removed_docs, tokenizer)\n",
    "# this is a list of lists, the numbers represent the index position of that word.\n",
    "# for instance, 33 means the 33rd word in the vocabulary\n",
    "# Notice the last document has 4 numbers, since it is a 4 word document: Could have done better.\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras toolkit\n",
    "from random import randint\n",
    "from numpy import array, argmax, asarray, zeros\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = int(len(tokenizer.word_index) * 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in GloVe Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def load_glove_vectors():\n",
    "    embeddings_index = {}\n",
    "    with open('glove.6B.100d.txt') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "embeddings_index = load_glove_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((VOCAB_SIZE, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: # check that it is an actual word that we have embeddings for\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9164"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import SimpleRNN, LSTM\n",
    "from keras.layers import Flatten, Masking\n",
    "# define model\n",
    "\n",
    "def make_binary_classification_rnn_model(plot=False):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "    model.add(SimpleRNN(units=64, input_shape=(1, MAX_SEQUENCE_LENGTH)))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize the model\n",
    "    model.summary()\n",
    "    \n",
    "    if plot:\n",
    "        plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "def make_lstm_classification_model(plot=False):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "    model.add(LSTM(units=32, input_shape=(1, MAX_SEQUENCE_LENGTH)))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize the model\n",
    "    model.summary()\n",
    "    \n",
    "    if plot:\n",
    "        plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 177, 100)          916400    \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 177, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 933,986\n",
      "Trainable params: 17,586\n",
      "Non-trainable params: 916,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_lstm_classification_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Changing Y variable can help output other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_label.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 'is_casual'\n",
    "labels=data_label[cols].values\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "labels = to_categorical(encoder.fit_transform(labels))\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels, test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2818 samples, validate on 314 samples\n",
      "Epoch 1/5\n",
      "2818/2818 [==============================] - 8s 3ms/step - loss: 0.5909 - accuracy: 0.6757 - val_loss: 0.5401 - val_accuracy: 0.7070\n",
      "Epoch 2/5\n",
      "2818/2818 [==============================] - 7s 2ms/step - loss: 0.5203 - accuracy: 0.7268 - val_loss: 0.5101 - val_accuracy: 0.7197\n",
      "Epoch 3/5\n",
      "2818/2818 [==============================] - 7s 2ms/step - loss: 0.4665 - accuracy: 0.7740 - val_loss: 0.4931 - val_accuracy: 0.7452\n",
      "Epoch 4/5\n",
      "2818/2818 [==============================] - 8s 3ms/step - loss: 0.4311 - accuracy: 0.7935 - val_loss: 0.4884 - val_accuracy: 0.7420\n",
      "Epoch 5/5\n",
      "2818/2818 [==============================] - 7s 2ms/step - loss: 0.4073 - accuracy: 0.8098 - val_loss: 0.4800 - val_accuracy: 0.7516\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "history = model.fit(X_train, y_train,validation_split = 0.1, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_fit_history(history):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "plot_fit_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784/784 [==============================] - 0s 424us/step\n",
      "random for is :67.16036772216547\n",
      "Accuracy: 79.336733\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "labels=data_label[cols].values\n",
    "rand = labels.sum()/len(labels)\n",
    "print(f'random for is :{max(rand,1-rand)*100}')\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 'is_modern'\n",
    "labels=data_label[cols].values\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "labels = to_categorical(encoder.fit_transform(labels))\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels, test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2818 samples, validate on 314 samples\n",
      "Epoch 1/5\n",
      "2818/2818 [==============================] - 7s 3ms/step - loss: 0.7348 - accuracy: 0.5667 - val_loss: 0.6095 - val_accuracy: 0.6529\n",
      "Epoch 2/5\n",
      "2818/2818 [==============================] - 7s 3ms/step - loss: 0.5840 - accuracy: 0.6987 - val_loss: 0.5946 - val_accuracy: 0.6624\n",
      "Epoch 3/5\n",
      "2818/2818 [==============================] - 8s 3ms/step - loss: 0.5470 - accuracy: 0.7232 - val_loss: 0.5863 - val_accuracy: 0.6688\n",
      "Epoch 4/5\n",
      "2818/2818 [==============================] - 7s 2ms/step - loss: 0.5268 - accuracy: 0.7417 - val_loss: 0.5666 - val_accuracy: 0.6815\n",
      "Epoch 5/5\n",
      "2818/2818 [==============================] - 7s 2ms/step - loss: 0.5143 - accuracy: 0.7420 - val_loss: 0.5672 - val_accuracy: 0.6943\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "history = model.fit(X_train, y_train,validation_split = 0.1, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784/784 [==============================] - 0s 421us/step\n",
      "random for is :53.166496424923395\n",
      "Accuracy: 73.214287\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "labels=data_label[cols].values\n",
    "rand = labels.sum()/len(labels)\n",
    "print(f'random for is :{max(rand,1-rand)*100}')\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for binary rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 177, 100)          916400    \n",
      "_________________________________________________________________\n",
      "masking_2 (Masking)          (None, 177, 100)          0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 64)                10560     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 928,034\n",
      "Trainable params: 11,634\n",
      "Non-trainable params: 916,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_binary_classification_rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 'is_modern'\n",
    "labels=data_label[cols].values\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "labels = to_categorical(encoder.fit_transform(labels))\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels, test_size=0.2,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2818 samples, validate on 314 samples\n",
      "Epoch 1/5\n",
      "2818/2818 [==============================] - 5s 2ms/step - loss: 0.6342 - accuracy: 0.6306 - val_loss: 0.6323 - val_accuracy: 0.6592\n",
      "Epoch 2/5\n",
      "2818/2818 [==============================] - 4s 1ms/step - loss: 0.5713 - accuracy: 0.6945 - val_loss: 0.6074 - val_accuracy: 0.6624\n",
      "Epoch 3/5\n",
      "2818/2818 [==============================] - 4s 1ms/step - loss: 0.5382 - accuracy: 0.7310 - val_loss: 0.5931 - val_accuracy: 0.6720\n",
      "Epoch 4/5\n",
      "2818/2818 [==============================] - 4s 1ms/step - loss: 0.5152 - accuracy: 0.7395 - val_loss: 0.6193 - val_accuracy: 0.6688\n",
      "Epoch 5/5\n",
      "2818/2818 [==============================] - 4s 1ms/step - loss: 0.4928 - accuracy: 0.7608 - val_loss: 0.6031 - val_accuracy: 0.6783\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "history = model.fit(X_train, y_train,validation_split = 0.1, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784/784 [==============================] - 0s 315us/step\n",
      "random for is :53.166496424923395\n",
      "Accuracy: 70.280612\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "labels=data_label[cols].values\n",
    "rand = labels.sum()/len(labels)\n",
    "print(f'random for is :{max(rand,1-rand)*100}')\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building model using simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers import Flatten, Masking\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "model.add(SimpleRNN(units=64, input_shape=(1, MAX_SEQUENCE_LENGTH)))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 177, 100)          916400    \n",
      "_________________________________________________________________\n",
      "masking_3 (Masking)          (None, 177, 100)          0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 64)                10560     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 929,106\n",
      "Trainable params: 12,706\n",
      "Non-trainable params: 916,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#compile model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "#plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 'is_modern'\n",
    "labels=data_label[cols].values\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "labels = to_categorical(encoder.fit_transform(labels))\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3524/3524 [==============================] - 5s 1ms/step - loss: 0.6429 - accuracy: 0.6453\n",
      "Epoch 2/20\n",
      "3524/3524 [==============================] - 5s 1ms/step - loss: 0.5726 - accuracy: 0.7009\n",
      "Epoch 3/20\n",
      "3524/3524 [==============================] - 4s 1ms/step - loss: 0.5431 - accuracy: 0.7202\n",
      "Epoch 4/20\n",
      "3524/3524 [==============================] - 5s 1ms/step - loss: 0.5211 - accuracy: 0.7375\n",
      "Epoch 5/20\n",
      "3524/3524 [==============================] - 5s 1ms/step - loss: 0.4951 - accuracy: 0.7639\n",
      "Epoch 6/20\n",
      "3524/3524 [==============================] - 4s 1ms/step - loss: 0.4695 - accuracy: 0.7755\n",
      "Epoch 7/20\n",
      "3524/3524 [==============================] - 4s 1ms/step - loss: 0.4440 - accuracy: 0.7917\n",
      "Epoch 8/20\n",
      "3524/3524 [==============================] - 5s 1ms/step - loss: 0.4229 - accuracy: 0.8070\n",
      "Epoch 9/20\n",
      "3524/3524 [==============================] - 5s 1ms/step - loss: 0.3861 - accuracy: 0.8272\n",
      "Epoch 10/20\n",
      "3524/3524 [==============================] - 4s 1ms/step - loss: 0.3642 - accuracy: 0.8388\n",
      "Epoch 11/20\n",
      "3524/3524 [==============================] - 4s 1ms/step - loss: 0.3349 - accuracy: 0.8581\n",
      "Epoch 12/20\n",
      "3524/3524 [==============================] - 4s 1ms/step - loss: 0.3056 - accuracy: 0.8757\n",
      "Epoch 13/20\n",
      "3524/3524 [==============================] - 4s 1ms/step - loss: 0.2660 - accuracy: 0.8927\n",
      "Epoch 14/20\n",
      "3524/3524 [==============================] - 4s 1ms/step - loss: 0.2489 - accuracy: 0.8956\n",
      "Epoch 15/20\n",
      "3524/3524 [==============================] - 5s 1ms/step - loss: 0.2507 - accuracy: 0.8902\n",
      "Epoch 16/20\n",
      "3524/3524 [==============================] - 5s 1ms/step - loss: 0.1978 - accuracy: 0.9188\n",
      "Epoch 17/20\n",
      "3524/3524 [==============================] - 5s 1ms/step - loss: 0.2030 - accuracy: 0.9132\n",
      "Epoch 18/20\n",
      "3524/3524 [==============================] - 5s 1ms/step - loss: 0.1564 - accuracy: 0.9398\n",
      "Epoch 19/20\n",
      "3524/3524 [==============================] - 4s 1ms/step - loss: 0.1531 - accuracy: 0.9387\n",
      "Epoch 20/20\n",
      "3524/3524 [==============================] - 4s 1ms/step - loss: 0.1589 - accuracy: 0.9407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1758db978>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392/392 [==============================] - 0s 546us/step\n",
      "random for is :53.166496424923395\n",
      "Accuracy: 66.581631\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "labels=data_label[cols].values\n",
    "rand = labels.sum()/len(labels)\n",
    "print(f'random for is :{max(rand,1-rand)*100}')\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
